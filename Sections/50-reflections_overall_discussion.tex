\section{Reflections \& Overall Discussions}
\label{sec:reflections}
This section explores the broader implications of the findings, summarizing both the results and their significance in the development of comparable systems. It evaluates the feasibility of the approaches presented in this thesis as viable solutions to the addressed problems. For coherence, the research questions introduced at the beginning are revisited and answered conclusively in this section.

\subsection{Privacy in Images}
\label{sec:privacy_in_images}
The developer has complete control over post-analysis image handling in systems with on-device processing. The most privacy-preserving method is to delete the image after analysis and transmit only the anonymized results in form of positional coordinates of the persons. However, image obfuscation may sometimes be preferable. While deleting the images is advantageous when the technology is proven or some level of analysis error is tolerable, the retaining obfuscated images facilitates easier technology development and verification, as verification of analysis results is possible. Additionally, obscuring images can provide transparency about the origin of analysis results, which may be beneficial in situations where end-users seek to verify and understand the data. Techniques for image obfuscation, such as blurring, were discussed, noting that blurring the entire image is a straightforward method to face blurring to avoid relying on facial recognition to work perfectly.

\citeauthor{ed2012privacy_review}s findings suggests blurred images are not be considered privacy preservant (\citeyear{ed2012privacy_review}). However, one could argue the results of this study are invalid for multiple reasons: 
\begin{itemize}
    \item Public perceptions of privacy have evolved since 2012, potentially diminishing the study's relevance.
    \item Internal validation: The design of the questionnaire may have biased participants to perceive blurred videos as more invasive than they might actually be. This could be because other, more privacy-preserving methods were presented alongside blurred videos, leading participants to comparatively view blurring as the least effective option. Had blurred videos been evaluated in isolation, they might have been judged as adequately privacy-preserving.
    \item External validation: The demographic profile of the participants might constrain the studys relevance to regions like Scandinavia, where a generally higher trust in public institutions might lead to greater acceptance of obfuscation methods. Exploring how demographic variations influence perceptions of privacy protection methods, however, falls outside the scope of this thesis.
\end{itemize}

Having discussed the nuances of privacy preservation in image processing, let us now turn our attention to the implications of using third-party services and products, which present a different set of challenges and considerations.

\subsection{Third-Party Services \& Products}
\label{sec:discuss_thirdparty}
This subsection delves into the pros and cons of integrating third-party services and products into object detection systems, highlighting the trade-offs between ease of development and control over data and functionality. As discussed in Section \ref{sec:thirdparty}, third-party services offer significant advantages over creating a solution from the bottom. However, these advantages must be balanced against several potential drawbacks.
\paragraph{Potential Drawbacks of Utilizing Third-Party Services or Products}
\begin{enumerate}
    \item \textbf{Control Over the System:} Developing an in-house application provides unparalleled customization opportunities, from software architecture to data processing and system integration. Such autonomy allows for system optimizations specific to performance and operational needs, and independence from the continuance and performance of external services.
    
    \item \textbf{Data Privacy and Security:} On-device processing ensures data remains confined to the device, thereby enhancing security and privacy. Although some providers, such as Roboflow, offer local deployment, this often requires a costly business-level subscription.
    
    \item \textbf{Cost Efficiency:} While third-party services may reduce upfront development costs, they can incur ongoing charges like subscription fees, usage rates, and costs for premium features or enhanced support. For large or long-term projects, these expenses can be substantial.
    
    \item \textbf{Performance Optimization:} By owning the entire system pipeline, one can tailor both hardware and software for optimal performance, achieving faster processing speeds and reduced latency that third-party services may not match.
    
    \item \textbf{Scalability and Integration:} Custom solutions facilitate easier scaling and integration with existing IT infrastructure, aiding in the seamless expansion of data workflows and supporting business growth without the constraints of external platforms.
\end{enumerate}

In conclusion, while third-party services can accelerate development in the field of object detection, it is crucial for researchers and practitioners to meticulously consider these aspects, particularly in sensitive applications like person detection where privacy is paramount. Exploring both proprietary and third-party solutions will provide a balanced perspective on flexibility, control, and innovation potential.

While third-party services offer certain benefits and drawbacks, the application of person localization systems, especially in environments like museums, presents unique opportunities that affect various stakeholders differently.

\subsection{Applicability of Person Positioning Systems}
\label{sec:applicability_person_localization}
Here, we explore how person localization systems can be applied in cultural settings, examining the differing priorities and potential benefits for various stakeholders such as curators and administrators. 

Curators may prioritize the enhancement of visitor engagement and educational experiences. Person positioning systems can provide valuable data on visitor traffic patterns, dwell times, and interest areas, enabling curators to optimize exhibit layouts and tailor informational content to visitor behavior. This data-driven approach can significantly enhance the educational impact of exhibits and improve overall visitor satisfaction.

On the other hand, administrators might focus more on operational efficiencies and security enhancements. The insights gained from person localization technologies can streamline staffing needs, enhance security monitoring, and manage crowd control more effectively. By understanding peak visitation times and the flow of visitor movement, administrators can allocate resources more efficiently, potentially reducing operational costs and improving the safety and comfort of museum environments.

Beyond the practical applications in cultural institutions, the development of person localization systems also raises profound ethical questions, particularly concerning public surveillance and data privacy.

\subsection{On the Ethicality of Person Positioning Systems Development}
\label{sec:discussion_ethics_localization_tech}
This subsection discusses the ethical considerations and societal perceptions related to the deployment of person localization systems, particularly in the context of public surveillance and privacy. History has shown Kant's categorical imperative to function as a guiding principle in smaller groups, but it often falters in larger societies where in- and out groups are forming. This is apparent, since wars and failure to provide basic humanitary aid to those in need is still an issue in global society. Enabling further mass public control through automated person localization devices raises significant ethical concerns. 

\paragraph{Public Surveillance Perceptions}
People of the general public often argue against mass surveillance, stating it is inherently wrong without being able to fully articulate the consequences. As seen in Section \ref{sec:smarthomeprivacy}, these objections often give way when convenience outweighs privacy concerns. This constitutes a need for a stronger motivation to uphold the individual privacy, once technology capabilities surpass these desires. Most agree to surveillance technologies in public spaces for security reasons, but the mass general public has also accepted having devices in our homes and pockets listening for a "Hi Siri", "Okay Google", or "Alexa", which displays a gradual increase in acceptance of devices which are potentially chipping away at our privacy. The potential for more intrusive technologies that track individual movements, gestures and activities within private spaces poses even greater risks to personal privacy. 

\paragraph{The Deontological Perspective and Its Limitations}
The statement by Redmon, "[...] I bought in to the myth that science is apolitical and research is objectively moral and good no matter what the subject is", reflects the limitations of a purely deontological approach\footnote{Deontological approach, meaning the morality of an action should be based on whether the action itself is right or wrong.} to ethics in technology. This perspective often overlooks the broader impacts of scientific advancements.

\paragraph{Balancing Benefits and Risks: A Utilitarian Approach}
Conversely, utilitarianism suggests that we must weigh the potential benefits of e.g. increased safety, efficiency, and convenience against the risks to privacy, individual freedoms, and other negative impacts. The key challenge for developers and policymakers is to maximize the net positive impactâ€”enhancing societal well-being while minimizing adverse outcomes. This perspective underscores the importance of impact assessments and ethical considerations in the development and implementation phases of these technologies.

\paragraph{Modern Philosophical Contributions to AI Ethics}
Contemporary philosophical discourse, as highlighted by thinkers like Harris, Russell, and Yudkowsky, stresses the importance of effective governance and informed regulation of AI technologies. These discussions are particularly relevant in the context of developing regulatory frameworks that can manage the ethical challenges posed by advanced technologies without stifiling innovation.

\paragraph{Conclusions on the Ethicality Person Positioning Systems Development}
In conclusion, the ethical deployment of computer vision technologies requires a multi-faceted approach. A deontological stance would be to not accept visual person localization technologies, but such positions often struggle to counteract the compelling convenience these technologies offer. Instead, a more robust ethical framework must be adapted by society. The utilitarian principles, which call for maximizing societal benefits while minimizing harms, and modern philosophical discourse on AI regulation highlight the necessity for informed policymaking. Since we can not expect every person to scrutinize the potential consequences of every new IoT device, we need governmental control to decide which technologies can make it into our homes. This control through regulations also allow for the heuristic of blindly trusting devices on the market, easing the decision-making process and reducing public resources spent individually to investigate products for their privacy characteristics. This is why products need CE marking exists, to ensure consumers the product conforms with European health, safety, and environmental standards. 

The necessity for regulation becomes particularly evident when considering societal behaviors that undermine collective well-being; continued littering, even though we have proof of trash destroying ocean life, and people choosing to not recycle their metals, even though we have scientific proof of Earth warming at a higher rate than ever previously measured\footnote{The dean of Information Technology and Electrical Engineering at NTNU mistakenly claimed Earth's temperature was higher than ever before, at a speech given to the graduating students of 2024. This is a common misconception, even among the highly educated population.}. These continued behaviours advocate for stronger governmental control, capable of rewarding well behaved citizens or sanctioning the ill behaved. This may motivate the use and implementation of public control technologies which may be capable of identifying undesired behaviour. The person localization systems discussed in this thesis is meant for visitor behaviour analysis in cultural insitutions, but could be extended to such applications. Redmon's development of the YOLO algorithm which was later used as a military weapon exemplifies the actual use of technological advancements can diverge significantly from their intended purposes. Regulations are necessary in a well-functioning society for controlling citizen behaviour, and they are necessary in a well-functioning society for controlling the crowd-control capabilities of future data science and computer vision technologies may facilitate.

Having examined the ethical landscape surrounding person localization technologies, we now shift focus to the practical aspects of system implementation, specifically the selection of appropriate hardware and software.

\subsection{Hardware and Software Choices}
\label{sec:hardware_choices}
In this subsection, we review the hardware and software choices made for the project, assessing their effectiveness and the lessons learned that could inform future projects. The selection of hardware and software for this project was made based on availability, cost, and compatibility with the project goals. While some choices might not have been optimal in hindsight, they were adequate for the project's scope. For instance, the chosen hardware handled the low-light conditions of the aquarium setting satisfactorily, and the software solutions provided the necessary functionality without excessive complexity. However, it is always valuable to reflect on these choices to identify potential improvements for future projects. It is likely that the advancements in mobile phone technology eventually will translate into more efficient low-level systems and better suited hardware for visual mobile systems.

There are many options for sensors to detect the presence and positions of persons in a room (IPS). The option to use a visual sensor was due to the opportunity to extend the application to perform more sophiticated tasks which are inaccessible to other sensors. These applications could include fall detection, approximation of age, emotion detection, and pose estimation. These improvements could be crafted by improving the software, not needing to reconstruct hardware or change the already deployed cameras. The extension of the person localization system to detect falls in public spaces where individuals sometimes travel alone could have the potential to save lives. The option to focus on visual sensors and the applicability in terms of ethicality and practical dilemmas was motivated by this extendability to meaningful and helpful devices that may save lives in the future\footnote{A clear application of these devices once the ethical principles and on-device processing procedures of deleting the image post-analysis would be to have them detect if an elderly person living at home has fallen, has taken his/her medications, or if the person has stayed in bed for longer than a pre-defined maximum period.}. 

\paragraph{Programming Language}
The chosen programming language of Python is typical for data science projects because of the broadly adopted libraries for computer vision, data management and machine learning. This motivated the option to write the code for the project in Python. Using a programming language closer to machine code, i.e. C, C++, or Rust, would prove beneficial to optimize device efficiency and speed and is a consideration that could have been taken more seriously if the goal of the application was real-time object detection or similar. Micropython could've been an option, but was foregone as there was no need for it since the Raspberry PI 3 is more than powerful enough to run regular Python.  

After addressing the technical decisions regarding hardware and software, it's crucial to reflect on the results of our system's implementation and how it meets the project's research questions and objectives.

\subsection{Results}
\label{sec:results_discussion}
This subsection presents a detailed discussion of the performance metrics used to evaluate the object detection models, highlighting the implications of these findings on model selection and system effectiveness. 

The evaluation metrics employed, COCO AP and Vary Both AP, yielded similar rankings for the tested models, suggesting that the choice of metric is not a crucial factor in model selection for this project. However, COCO AP is particularly beneficial as it mirrors the practical demands of object detection systems, where confidence thresholds are fixed to optimize real-world performance. This feature makes COCO AP a more representative metric for gauging how models might perform post-deployment.

\paragraph{Impact of Fine-Tuning on Performance}
The fine-tuning process, intended to refine model performance by adjusting model parameters to better fit specific datasets, paradoxically led to poorer outcomes. This degradation in performance could stem from several factors:
\begin{itemize}
    \item The evaluation setup for the model accuracies was not tested sufficiently to completely outrule biased results. This might have led to results that does not reliably reflect the impact of dataset quality and relevance.
    \item Potential missteps in the training process, such as inappropriate training protocols or suboptimal configurations for single-class inference, could have skewed the results.
    \item The architectural decisions regarding the number of anchors and layers in the neural networks may not have been optimized for the scenarios tested, resulting in pre-trained model weights being more optimized to the scenario. Adjusting these factors could potentially enhance model performance, suggesting that future studies should explore more nuanced modifications beyond simple fine-tuning without regards to other factors.
    \item The training process contained too few instances of persons. Either the datasets for fine-tuning would need to contain a higher number of relevant instances for the model to learn the necessary features, or it would need to train for many more epochs. As previously stated in the Results section, we already identified we hit a local maximum of model accuracy when it was fine-tuned on FIMUS \textit{Inconsistent} for 20 and 70 epochs. As a dataset with few instances per image and a relatively low number of images, the models that are fine-tuned on \textit{Inconsistent} may necessitate more training epochs to converge on better model weights. This is still the case altought the model will already know basic features from pre-training the backbone.
\end{itemize}

Freezing the backbone was most likely not the cause to worsened performance. The models were trained on the COCO dataset, which contains a vast amount of images and 80 classes, including person. The high accuracy of the standard YOLOv9 witness there's no issues in an unoptimized model to infer correctly on the data.  

Investigating the effects of model fine-tuning on varying degrees of dataset relevancy remains an interesting topic for the future. Leveraging neural network search techniques and comprehensive hyperparameter tuning could yield more definitive insights with regards to how well the most optimized model may perform, should the technology be used for an application where its performance is critical.

\paragraph{Reevaluation of Metrics}
Initial assessments included a range of performance metrics, such as average precision (AP), average recall (AR), and F1-scores. Despite the initial decision to prioritize AP50-95 based on its prevalence in the literature as a critical performance indicator, this decision was revisited. Upon reflection, incorporating AR and F1 scores into the final analysis was deemed essential, especially since the balance between accurately detecting targets (precision) and minimizing missed detections (recall) is crucial in many person localization applications. The recalculated results, incorporating a balanced consideration of both precision and recall, facilitated a more nuanced discussion about the performance of the models across different test set configurations.

\subsection{Research Questions}
\label{sec:discussion_research_q}
The research questions have been answered throughout the sections of this master thesis. However, finding the answers to one specific question of interest in a 70+ pages thesis may be difficult. To alleviate this, concrete, short answers are provided in the list below to quickly summarize the findings of this thesis. 

\begin{enumerate}
    \item What are some privacy risks associated with traditional person localization systems in public spaces, and how may a system mitigate these privacy concerns?
    
    Privacy risks include unauthorized data collection, misuse of personal data, and lack of transparency in data handling. On-device processing mitigates these concerns by keeping data localized, enhancing security and privacy, and allowing users greater control over their data. 
    
    \item How does the validity of object detection model evaluations change when using data specifically from the intended deployment environment compared to using generic datasets?

    Accuracy scores were much higher in the thesis project than evaluations of the same models on generic datasets. This illustrates how scientific model evaluation consider how well a model performs given a wide array of diverse images with several challenges, as opposed to a specific use case where many of the challenges are similar for all the images.  

    The validity of object detection model evaluations thus improves when using data from the intended deployment environment because the model is more precisely measured on it's ability to recognize specific features and variations present in the specific environment.
    
    \item What are some machine learning architectures suitable for object detection in a real-world deployment scenario?
    
    Multiple machine learning architectures are suitable for object detection. Section \ref{sec:object_detection} discussed multiple architectures, including those belonging to the category of \textit{traditional} machine learning. We've seen how which architecture is optimal depends on the preferred capabilities, such as the ability to fine-tune, speed, and accuracy. The YOLO series make for robust, efficient and easy-to-implement models, and the Co-DETR is the current best-performant, but the DETR series is possibly premature, more complex, and may be more difficult to implement.

    \item How do the performance metrics of object detection models compare when applied to different quality datasets?
    
    The consistent test sets (\textit{Consistent-1}, \textit{Consistent-2}, \textit{Consistent}) represent higher quality images and show variability in performance metrics (see \ref{tab:larger_test_set}). This indicates that even within high-quality images, different partitions with separate characteristics can affect performance metrics differently. Further, this advocates for having as large and diverse test set as possible. It should cover as many of the possible encounterable post-deployment scenarios an object detector may face. 

    The results also show different results for YOLOv3 and YOLOv9. For v3, the model performs best on the Inconsistent test-set, while this discrepancy is not as clear for the v9. For experiments aiming to find the best model, the test set is not selected in the way we are experimenting on the possibilities in this thesis project. Normally, the test set would be randomly selected from the available data. However, these results support the neccessity of a truly unbiased sampling of test data, as the test data itself may influence the choice of object detector model. 
\end{enumerate}

All the research objectives (see Section \ref{sec:research_objectives}) have been successfully achieved throughout the thesis. The comprehensive Structure section (\ref{sec:structure}) provides a roadmap, facilitating easy navigation to specific topics of interest within the document. Given this clear organization, a reiteration of the objectives and their fulfillment is deemed redundant and unnecessary. Readers seeking to verify the achievement of specific goals are encouraged to refer directly to the Structure section.

With the detailed results and responses to our research questions laid out, we now explore the broader implications of our findings, considering their impact on future technological developments and ethical considerations.

\newpage
\subsection{Broader Implications}
\label{sec:discussion_broader_context}
Finally, we consider the wider societal and ethical implications of our findings, reflecting on how they contribute to ongoing discussions about technology, privacy, and the role of AI in public spaces. The practical demonstation of basic-level computer vision technology in the project of this thesis plays a tiny role in advancing technology. This aligns with the stated objectives of this work, which include demonstrating, investigating, and assessing how the quality and relevance of datasets influence the fine-tuning of models. Through this, the provided insights are not only technically informative but also contextually relevant to the ongoing evolution of AI technologies.

The theoretical framework of this thesis and especially the privacy and ethical considerations convey a message of necessity for a thoughtful and comprehensive approach in developing and implementing technology. Developers bear a significant responsibility to ensure ethical performance, as the public often fails to recognize these issues and public regulations lag behind.

By upholding the aforementioned ethical standards and evaluating the potential outcomes of the project, we can justify the development and live experimentation of the system in this thesis project. Recognizing that the benefits to societal advancement and the contributions to ethical discourse make the project acceptable and valuable as a demonstration of the feasability and effectiveness of on-device person detection in a practical and realistic setting (Primary Objective 2). 

This discussion expands to the broader topic of using sensitive data for training AI models. META AI, specifically, has faced criticism for training their models on personal data without clear communication about their intentions, potentially retaining flexibility for future uses while avoiding misrepresentation. This practice highlights broader ethical concerns about transparency and trust in how AI companies manage and utilize sensitive data (\cite{ing2024metapersonvern}).

As mentioned in the Scope of this thesis (see \ref{sec:scope}), the scope of this project extends beyond the immediate technical implementation to a wider examination of its implications and applications. The deployment of the on-device detection system within the FIMUS aquarium illustrates a tangible application of theoretical concepts to practical settings. This showcased the feasibility and utility of edge computing in enhancing visitor analysis without compromising privacy. Another key advantage of the system is it's non-interference with the visitation experience by having the visitors wear custom devices or answer questionnaires as they move through the facilities. This system's ability to generate actionable insights through data visualization, like heat maps of visitor positions, provides empirical evidence of the technology's value, potentially improving visitor experiences and operational management.

A focal point of this thesis is the robust implementation of privacy-preserving methodologies, critical in garnering public trust and ethical approval for deploying surveillance technologies in sensitive environments. By prioritizing anonymization and minimal data retention, this project aligns with the highest standards of data ethics, reinforcing the importance of privacy considerations in the design and deployment of technological solutions.

Furthermore, this thesis addresses the critical topic of privacy preservation. The emphasis on anonymized data collection and the implementation of privacy-preserving measures underscore the ethical considerations essential for deploying such technologies in public spaces. This focus attempts to not let the technical advancements of the practical implementation come at the expense of individual privacy rights.