\section{Results}
\label{sec:results}
This chapter will present the results of the human detection and tracking system, including the system's performance in the museum environment, the effects of adding labeled images from the museum environment to the training dataset, and the system's ability to detect and track humans in real-time.

The FIMUS 2nd iteration images were used as the test set for all of the evaluations. This dataset consists of 295 images of similar light condition and image quality. They are the closest representation of the images the device will be capturing in the experimental setting. All images are of 3264x2464 resolution (which is the maximum for the hardware).

The following models were tested:

"Standard models:"
	Yolov3
	Yolov9 using imgsz 640 for inference
	Yolov9 using imgsz 1280 for inference

"Specialized models:"
	FIMUS trained 5 epochs
	FIMUS trained 50 epochs
	CrowdHuman trained 5 epochs




\subsubsection*{Model Evaluation}
The full model evaluation jupyter notebook can be seen in appendix todo insert model evaluation ipynb.

For reproducibility, if the same experiment is to be performed, make sure to set confidence threshold at 0.1. The FIMUS 5, 15 and 50 epochs are models pretuned on the COCO dataset, and fine-tuned with the FIMUS 1st iteration dataset. These were all trained for and detected at imgsz 640. The CrowdHuman 5 Epochs model was fine-tuned with 5 epocs on the training/validation datasets of CrowdHuman. The rest of the models were not fine-tuned.

% 7.mai
\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.5} % Increase vertical padding
    \setlength{\tabcolsep}{1em}
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \rowcolor{gray!25}
        \textbf{Model} & \textbf{AP50} & \textbf{AP75} & \textbf{AP90} & \textbf{mAP50-95} \\ \hline
		Fimus Fine-Tuned 5 Epochs              & 0.97 & 0.89 & 0.67 & 0.88 \\ \hline
		Fimus Fine-Tuned 15 Epochs             & 0.94 & 0.67 & 0.44 & 0.71 \\ \hline
		Fimus Fine-Tuned 50 Epochs             & 0.94 & 0.75 & 0.51 & 0.77 \\ \hline
		CrowdHuman Fine-Tuned for 5 Epochs	   & 0.96 & 0.93 & 0.77 & 0.91 \\ \hline
		Standard Imagesize 320 		           & 0.92 & 0.85 & 0.62 & 0.84 \\ \hline
		Standard Imagesize 640 		           & 0.96 & 0.93 & 0.90 & 0.94 \\ \hline
		Standard Imagesize 1280		           & 0.97 & 0.93 & 0.79 & 0.92 \\ \hline
		YOLOv3 							       & 0.95 & 0.88 & 0.63 & 0.87 \\ \hline
		YOLOv3 on larger test-set\ref{sec:larger_test_set}			       					& 0.97 & 0.91 & 0.67 & 0.89 \\ \hline
		DETR ResNet50					   	   & 0.32 & 0.29 & 0.17 & 0.28 \\ \hline	
		DETR ResNet101					       & 0.63 & 0.56 & 0.30 & 0.53 \\ \hline 
		DETR Confidence .90					   & 0.33 & 0.28 & 0.13 & 0.27 \\ \hline
		DETR Confidence .95					   & 0.98 & 0.87 & 0.41 & 0.83 \\ \hline
		DETR Confidence .97					   & 0.99 & 0.89 & 0.43	& 0.85 \\ \hline
		DETR Confidence .99					   & 0.99 & 0.92 & 0.48 & 0.88 \\ \hline
		Yolov9 Confidence .50				   & 0.99 & 0.97 & 0.95 & 0.98 \\ \hline
		YOLOv9 Confidence .50 on bigger test-set\ref{sec:larger_test_set} & 0.98 & 0.96 & 0.93 & 0.96 \\ \hline
    \end{tabular}
    \caption{\centering Performance Metrics of Object Detection Models on 294 images. }
    \label{tab:performance_metrics}
\end{table}

\paragraph{Larger Test-set}
\phantomsection
\label{sec:larger_test_set}
As seen in the table above, adding in the images collected during the 3rd iteration reduced the scores for YOLOv9 while increasing the scores of the YOLOv3. The fact the scores change when adding more test data illustrates illustrate that we have not yet reached a level of test data where the scores converge. More test data should be added to further increase validity of the results.
% 7.mai
% \begin{table}[H]
%     \centering
%     \renewcommand{\arraystretch}{1.5} % Increase vertical padding
%     \setlength{\tabcolsep}{1em}
%     \begin{tabular}{|l|c|c|c|c|}
%         \hline
%         \rowcolor{gray!25}
%         \textbf{Model} & \textbf{AP50} & \textbf{AP75} & \textbf{AP90} & \textbf{mAP50-95} \\ \hline
% 		FIMUS 5 Epochs              & 0.97 & 0.89 & 0.67 & 0.87 \\ \hline
% 		FIMUS 15 Epochs             & 0.94 & 0.67 & 0.44 & 0.71 \\ \hline
% 		FIMUS 50 Epochs             & 0.95 & 0.75 & 0.51 & 0.76 \\ \hline
% 		CrowdHuman 5 Epochs         & 0.97 & 0.94 & 0.78 & 0.92 \\ \hline
% 		Standard Yolov3             & 0.97 & 0.90 & 0.65 & 0.87 \\ \hline
% 		Standard Yolov9 160         & 1.00 & 0.86 & 0.36 & 0.81 \\ \hline
% 		Standard Yolov9 320         & 0.99 & 0.93 & 0.70 & 0.91 \\ \hline
% 		Standard Yolov9 640         & 0.98 & 0.95 & 0.91 & 0.95 \\ \hline
% 		Standard Yolov9 1280        & 0.98 & 0.94 & 0.80 & 0.92 \\ \hline
% 		Standard Yolov9 c-converted & 0.96 & 0.92 & 0.81 & 0.91 \\ \hline
% 		Standard Yolov9 e-converted & 0.99 & 0.97 & 0.95 & 0.98 \\ \hline
%     \end{tabular}
%     \caption{\centering Performance Metrics of Object Detection Models. }
%     \label{tab:performance_metrics}
% \end{table}

The weights are available in multiple versions of the Yolov9 model. The typical versions are with regards to the sizes of the networks, where the standard for these results have been to use the largest. This is called 'yolov9-e' in the github repo\footnote{Yolov9 github repo: \href{https://github.com/WongKinYiu/yolov9}{https://github.com/WongKinYiu/yolov9}}. Additionally, after it's release, some

\label{sec:results_heatmaps}