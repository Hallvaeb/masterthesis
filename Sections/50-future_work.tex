\section{Future work}
\label{sec:proposed_future_work}
The future work section is dedicated to outlining and proposing some potential pathways for further exploration. The idea of this section is to provide some ideas for several objectives of future work, which can be done either by themselves, or as a single project with multiple of the following proposed directions of research.

\subsection{Highly specific training dataset}
    This idea is inspired by 2 elements: (1) Where's the Bear's (\cite{el2017WTB}) approach to artificially place many different bears collected online, in images captured by their cameras, to make more training data on bears for their specific application. (2) The fact that real world situations differ from the situations typically found in benchmark datasets, as mentioned in section sec:discussion ml techs. In section , we discuss research where a dataset with a different angle than what is typical in the benchmark datasets was created, which improved accuracy of the model.

    For this idea, the main task will be to explore how much data from a specific setting is needed for a tiny/light ML models to surpass the accuracy of a large ML model. E.g. could a light YOLOv3 with an hours-worth of obtained specific data for a given setting be more accurate, faster, and smaller than a ML model trained on a larger general purpose dataset such as the COCO\footnote{COCO: Common objects in context, introduced in section \textit{sec:performance benchmark}.} dataset? Could these light ML models \textit{surpass} the accuracy of SOTA machine learning models for low light settings? How does accuracy increase with the size of a specialized training dataset?
    
    These insights will not only prove relevant for the specific ML models tested, but may also provide more general knowledge about the use of specialized datasets for ML applications. 
    
    It can be challenging to research the impact of a highly specialized dataset for ML model training on the accuracy and efficiency of models due to the difficulty of obtaining labeled data. Identifying correct model inferences, and thus recognizing improvements in the models, requires knowledge of the ground truth. This will likely be obtained by manually annotating data. E.g. to know the improvement of the accuracy of a human detecting model, one must know how many humans are actually in the image to be able to verify the model's count. This is a complex task as the images may contain natural persons who have not consented to being in the images. Therefore, the research will need consenting persons to build the dataset.
    
    If the model is trained solely on specific data for the given use case, the ability to generalize the model to other use cases is lost, as it will only be highly efficient for the given elements in the images it has seen and may be confused by new shapes or objects it has not seen. 
    
    \paragraph{Proposed approach}
    Mount a visual edge device (hereby "the device") and build a dataset of images from a specific setting. These images need to be compliant with GDPR, and need to be sharp as they are to be used for ML model training. Therefore, only consenting persons must be in the images.
    
    Then, the edge device is deployed with a large, general purpose model (hereby "the large model") and inference is ran in a real-world setting in the same area. The individual privacy of the persons in these images may be preserved through blurring the images post-analysis.  
    
    While the large model is running, the obtained images from the specific setting may be annotated. This must be done in some way where a bounding box is set around each person in each image, i.e. the corner coordinates of each bounding box in the image is saved along with a reference to the image. The bounding boxes are what will be used to train the models, and the precision of the manual labor will thus influence the accuracy of the resulting model.
    
    This specialized, newly obtained dataset is then incorporated into a light ML model, and deployed to the device.
    
    The images and inferences from the large model on the device are now to be analyzed, finding the ground truth by looking at the blurred images and counting every person, and comparing these counts to the inferences of the model. Additionally, the bounding boxes of the blurred images should be manually evaluated if they seems sufficiently accurate or not (which will be a subjective measurement, which is okay as the measurement of model success will be mostly determined by achieving the correct count of people). 
    
    Finally, the data from the light, specialized ML model are collected and analyzed, and compared to the result of the large model.

    Then, the experiment may be extended in multiple directions depending on the results. One would be to obtain more specific data, to see how more data will further improve the accuracies of the lighter version model. Another is to see how using less specific data would impact the accuracies. Another would be to try with a completely different model, i.e. switching from a Faster R-CNN to YOLO, or to drop the feature pyramid networks in a Faster R-CNN to evaluate the impact on performance and accuracy.

    The primary objective of such an approach could be to deliver an assessment of the impact of usage of use case-specific data in machine learning model training.

\subsection{Robustness of detectors}
    Another direction could be to assess the accuracies and throughputs of three different ML algorithms in various conditions, e.g. with regards to light, scale, occlusion and pose. The models should all be trained with the same data, and deployed to the same type of hardware and the same environment. 
    
    Some of the questions to be addressed would be: Will there be a difference in which detector is the most accurate, dependent on conditions such as light, scale, occlusion and pose? Will Faster R-CNN be more accurate but slower than YOLO? Are SOTA detectors overfitted to data commonly found in benchmark datasets, and less accurate on real-world data? 
    
\subsection{Feasibility of implementing tiny ML models on cost-effective hardware}
    A third proposal would be to create optimized firmware for cost-effective hardware to perform the specific, repeating job of taking an image, performing model inference to count the number of people on the image, and sending the count to a remote server. How tiny in size may the models be for the inferences to be sufficiently accurate?

    The exploration of hardware, which is in constant state of improvement and change, may be outdated in a couple of years. However, finding the status quo with regards to a in 2024 may still be relevant for future endeavours. 

\subsection{Heat map implementation}
    The last proposal is to create a visualization of the data. This can most likely be done as a part of a bigger project. To visualize the counts for various areas in the image may be best visualized through a heat map of where the detected persons have been. Such visualization, taking advantage of the stationary nature of the visual edge device, may be beneficial as much data can be integrated into one image in a completely anonymous and privacy preservant way.

    