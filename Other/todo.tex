- Capture images for a specialized dataset.
	Set up camera with the right settings.
	Eksamen medisinstudenter 17. april.
		Få med Marinka som høy person...
		Jakob er lav... Og har en datter...
- Label the dataset to have ground truth data.
	Process prepared: 
		tiling-experiment/yolov9/config, detect
		tiling-experiment/find_images_without_predictions
			This gives a count and the images where yolov9 found no people...
		tiling-experiment/yolo_to_json_predictions
			This will give the predictions in json format so it can be imported to ls
		lsenv
		label-studio start
		add source path to local folder
		import predictions.json
			Now, if the paths in predictions_converter are correctly set up the predictions will all appear as separate predictions for each photo.
		select all, create annotations from predictions
		press the first of the images and go through the list, verifying the annotations
			predictions can remain
		manual label what has been missed.
		import the images resulting from "find_images_without_predictions" to annotate those as well
			Do this by syncing the folder, as directly importing the images will result in weird filenames of the annotations, complicating the conversion afterwards.
		export json minimum format
			The yolo format appears to have a bug, only exporting one annotation per image, ignoring where there are multiple annotations for a single image.
			This formats the bboxes in a non-normalized way. We have to do the inverse of predictions_converter to get them on the yolo format for training.
		run the 'json_to_yolo_labels.py' script to correctly convert back to yolo format 
		these images with their labels can now be put in a folder structure that is compatible with the training scripts of yolov9.
		- {dataset_name}
			- images
				- {image1.jpg}
				- {image2.jpg}
				- ...
			- labels
				- {image1.txt}
				- {image2.txt}
				- ...kz

- Train multiple detectors with the data.
	The dataset was split randomly into a training (80\%) and validation (20\%) set.
	Several versions of yolov9:
		1) standard, pretrained
		Trained with 1st dataset
			2) 100, 3) 500, 4) 1000 images
		Trained with 2nd dataset
			5) 100, 6) 500 images, 7) 1000 images
		8) Trained with all data available from both datasets

Variables in datasets
	- varying capture settings in 1st dataset
	- some images in no infrared
	- 1 person
	- 4 persons
	- other datasets: 
		crowdhuman 
		football players
		PMW

Jeg tenker ulike versjoner av yolov9 standard, crowdhuman, football players, pmw?, fimus
med ulike mengder av treningsdata skal testes med bilder fra fimus.

- Test on unseen data obtained from the devices.
	Will be okay-easy and quick to do and render results regarding validity of the dataset.

- Test on live data from detectors deployed on real-world live devices.
	Here we will have no ground truth... Counting live is subject to human error. 
	For each set of detector results over a week (or so), make a heat map to illustrate their detections. 
	Compare heat maps to see if differences in accuracy matters for an application tasked with finding the location of persons in a dark-lit indoors acquarium situation.

	Deployment of yolov3 and yolov9 with differing dependencies on the same device:
	Solution to dependencies issues? Docker, venvs?	Ask Erik? Amund? Go with venv because simplistic?

Hvor mange bilder trengs for at en maskin skal forstå hvor mennesker er på et bilde.

Ved levering:
- KI-deklareringsskjema

My project is to measure how much dataset specialization affects person detection accuracy.

punktum på slutten av image captions?
Captionalization of words in titles of sections, paragraphs

Kaller vi datasettene inconsistent og consistent?

Short for figurer for å fikse captions i ToC. 



Fikse AP calculations... Bevare det vi hadde som averager over begge. Regne ut COCO sin også, evt Pascal COCO sin methode. Så kan jeg diskutere fordeler og ulemper med metodene... 
‍The Precision-Recall curve is often presented in one of two ways: with a fixed IoU and a varying confidence threshold (i.e. PASCAL VOC challenge), or with a varying IoU and a fixed confidence threshold (i.e. COCO challenge).

Misconception #2: There is a standard way of computing mAP by third party libraries  
Reality: There is no consensus with respect to a unified way of computing mAP by third party libraries, hence you need to verify that your team, your customers and other collaborators are using the same definition across your experiments.

To make it crystal clear, there are at least two ways to compute mAP:

Pascal VOC: the mAP as presented in the Pascal VOC challenges was the standard approach, this definition uses a single IoU=0.5, and averages over multiple confidence thresholds.
COCO challenge: a new variant (and the current dominant one) to compute mAP was introduced in this challenge. The mAP is averaged over 10 IoU thresholds (0.5, 0.55, …, 0.95) while using a single confidence threshold. There might be some cases where users average over both thresholds: they average over multiple confidences, and also over multiple IoU thresholds, hence it's crucial that your mAP definition is consistent across your experiments.

Key takeaway: Avoid inconsistent results in your detector performance by making sure your team is using the same mAP library so that you use the same mAP definition across your experiments.

In case you are curious, some of the most used libraries in Pytorch such as TorchMetrics and pycocotools compute mAP as follows:

AP and AR are averaged over multiple Intersection over Union (IoU) values. Specifically we use 10 IoU thresholds of .50:.05:.95.