- Capture images for a specialized dataset.
	Set up camera with the right settings.
	Eksamen medisinstudenter 17. april.
		Få med Marinka som høy person...
		Jakob er lav... Og har en datter...
- Label the dataset to have ground truth data.
	Process prepared: 
		tiling-experiment/yolov9/config, detect
		tiling-experiment/find_images_without_predictions
			This gives a count and the images where yolov9 found no people...
		tiling-experiment/yolo_to_json_predictions
			This will give the predictions in json format so it can be imported to ls
		lsenv
		label-studio start
		add source path to local folder
		import predictions.json
			Now, if the paths in predictions_converter are correctly set up the predictions will all appear as separate predictions for each photo.
		select all, create annotations from predictions
		press the first of the images and go through the list, verifying the annotations
			predictions can remain
		manual label what has been missed.
		import the images resulting from "find_images_without_predictions" to annotate those as well
			Do this by syncing the folder, as directly importing the images will result in weird filenames of the annotations, complicating the conversion afterwards.
		export json minimum format
			The yolo format appears to have a bug, only exporting one annotation per image, ignoring where there are multiple annotations for a single image.
			This formats the bboxes in a non-normalized way. We have to do the inverse of predictions_converter to get them on the yolo format for training.
		run the 'json_to_yolo_labels.py' script to correctly convert back to yolo format 
		these images with their labels can now be put in a folder structure that is compatible with the training scripts of yolov9.
		- {dataset_name}
			- images
				- {image1.jpg}
				- {image2.jpg}
				- ...
			- labels
				- {image1.txt}
				- {image2.txt}
				- ...kz

- Train multiple detectors with the data.
	The dataset was split randomly into a training (80\%) and validation (20\%) set.
	Several versions of yolov9:
		1) standard, pretrained
		Trained with 1st dataset
			2) 100, 3) 500, 4) 1000 images
		Trained with 2nd dataset
			5) 100, 6) 500 images, 7) 1000 images
		8) Trained with all data available from both datasets

Variables in datasets
	- varying capture settings in 1st dataset
	- some images in no infrared
	- 1 person
	- 4 persons
	- other datasets: 
		crowdhuman 
		football players
		PMW

Jeg tenker ulike versjoner av yolov9 standard, crowdhuman, football players, pmw?, fimus
med ulike mengder av treningsdata skal testes med bilder fra fimus.
	Måle/sammenligne accuracy

- Test on unseen data obtained from the devices.
	Will be okay-easy and quick to do and render results regarding validity of the dataset.

- Test on live data from detectors deployed on real-world live devices.
	Here we will have no ground truth... Counting live is subject to human error. 
	For each set of detector results over a week (or so), make a heat map to illustrate their detections. 
	Compare heat maps to see if differences in accuracy matters for an application tasked with finding the location of persons in a dark-lit indoors acquarium situation.

	Deployment of yolov3 and yolov9 with differing dependencies on the same device:
	Solution to dependencies issues? Docker, venvs?	Ask Erik? Amund? Go with venv because simplistic?

Hvor mange bilder trengs for at en maskin skal forstå hvor mennesker er på et bilde.



Ved levering:
- KI-deklareringsskjema





My project is to measure how much dataset specialization affects person detection accuracy.
