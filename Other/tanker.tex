\section{Tanker}
Skaffe en divers gruppe samtykkende mennesker med flere former for bekledning; luer, caps, jakke, tskjorte. Eller bare meg, med masse ulike klær.

Image segmentation, hente ut utsnittet av menneskene;

Plassere disse menneskene på tilfeldige steder rundt i et område;

Trene datasettet på disse utsnittene;

Gjør vi det da bedre for det gitte området?


Kan dette streamlines, prosedurebeskrivelse:

1. Sett opp enheten fysisk der den skal sitte. 

2. Et bilde der det er ingen mennesker tas, og bruker verifiserer bildeområdet. Dette bildet beholdes og brukes senere i prosessen.

2. Bruker trykker på knapp: "start datainnsamling".

3. 10min: bruker/brukere beveger seg rundt, ulike posisjoner, ulike bekledning.

4. Datainnsamling fullført, enheten forsøker å hente ut menneskene i disse bildene, bruker korrigerer. Her vil det være nødvendig med et verktøy for bounding box der brukeren kan korrigere for hvert bilde som er blitt tatt.

5. Når utsnittene er korrekt satt, lagres de lokalt, og brukeren ferdig.

6. Enheten dupliserer det tomme bildet 100 000 ganger, og setter inn tilfeldige utsnitt på tilfeldige plasser i bildene. 

7. ML modell (vision transformer?) trenes på generelt dataset, så fyller på med disse bildene.


Highlights:

1. The produced dataset will be highly specialized to a specific area, thus possibly greatly increasing the accuracy for difficult scenarios.

2. The dataset will not be optimized to different situations, but the segmented people may possibly be applicable to insertion in similar environments (with respect to light conditions). 

3. Must be same light conditions in the whole operational period of the ML model.

4. Experiment to see if the produced data alone can suffice for similar accuracies on smaller models without the need of massive training, thus shifting the workload of machine learning to specialized data acquizition rather than computational heavy model training.


Questions:
Do the cameras need to be RGB? Why need colors? More information for the space if we see greygscale? Better accuracy with colors?

Use Raspberry Pi Black camera?
https://medium.com/@joehoeller/object-detection-on-thermal-images-f9526237686a

Må tilpasse yolov9 til antall klasser?


Hypothesises regarding museums:
Hvad er det der museumet er interessert i at finde ud?
    - Hvor lenge en person ser på hver fisk




Baseline creation:
1. Have the device function as normal, try to detect people. 
2. Improve it so it has detection algorithm more optimized for short distance detection
3. If not well: specialized dataset. If well: heatmap and efficiency.

Specialized dataset:
1. Setup camera and make it send images to a google bucket. 
2. Object detect persons on device, then send count and image to bucket.
3. Create code to obscure image before sending


Notes are taken on Work-Confluence as well for easier access and transparency with coworkers. 



TODO definisjoner/introduksjoner til begrep og teknologier som brukes i oppgaven:
IoT
Edge computing
    On-device processing
    Privacy-preserving
Computer vision
Object detection
    Artificial neural network
    Yolov9
    Pre-trained model
    Dataset
    Labeling
Raspberry Pi
    Picamera
Visitor analytics
    Fiskeri og Søfartsmuseet

Spørsmål til veileder:
Burde jeg ha med matematisk/dyp forklaring av hvordan picamera fungerer?
Er det viktig å inkorporere noe om hvordan en kunstig nevral nettverk fungerer, eller holder det med en kort forklaring av at det er en algoritme som lærer fra treningsdata for å kunne utføre en spesifikk oppgave?

Shutter speed på 80 000 microseconds kan ta 12.5 billeder på 1 sekund. 


Samtale med Peter:
Vi burde vel heller bare ha et bedre kamera?

In the case of a better camera, the detector algorithms would also need to be trained on the image output of the camera, as the quality of the image would be different. However, during training, it is common practice to augment the images to simulate different conditions, so it is possible that the model would be able to detect people in the new images. This can be done by methods such as Mosaic augmentation or Mixup. Mosaic augementation of training data is where multiple images are combined into one. This created more training data, and let's the model learn different conditions than what is seen from the standard training data. As is the case with Mixup as well, where multiple images are averaged together to create a new image. These ways of data augmentation are ways to generalize a models capabilities beyond what may already be learnt from training data and is one argument to why a camera capturing images in a different format than 'rbg' or 'bgr' may still be useful in an object detecting application.

On the contrary, for a infrared-detecting camera, there also needs to be an infrared source. Humans emit some infrared light, but not enough to be detected by a camera at a distance. Therefore, a infrared source would need to be installed in the acquarium, worsening the product deployability and development.


Tried to download/use model from Roboflow, but either image has to be sent to an API which would not retain privacy, or the device has to host an API itself to run the inference... Seems unlikely to be the most preferable solution, as the device would have to set up the service and run it locally.

Possibly an interesting solution would be to do this with multiple devices. This supports the master-slave pattern of having multiple weaker computers and have them send to the stronger unit.
Setting up private TCP connection between the weaker units and the strong unit and have the images sent to the stronger, so it can detect on them and send information etc. How many weak units do we need in order to make it profitable to have a strong GPU unit to do the processing? This whole systems sounds to be a complicating setuo process, not making the product modular and easy-to-use. Includes a lot of connection/networking to make the weaker units find and connect to strong, physically close device.

This task would mean setting up a strong device to host a network to which the weak units might connect to, and send images to. The issue is whenever images are sent, a lot of transmission is used... But the model takes image input size of 416x416. Would it be similar to just downscale the image before sending, or would this give the model less detail to work with?

Will now run several models on datasets from the web, i.e. the CrowdHuman dataset to see their accuracies. Will then deploy the models to device in aquarium to see if the best-performing model is an option in terms of size and inference speed. If it is preferable, I will attempt to increase it's accuracy by accumulating and annotating a specialized dataset for that setting, and training the final layers on the data. Can this be done with a



A lot of time was spent setting up the arrangement in the aquarium and having the raspberry pi communicate correctly. The dataset collection process was also time-consuming, as it required the aquarium to be empty of non-consenting persons. Precautions were made in order not to upset the aquarium staff. This included not capturing outside opening hours, and not asking the visitors themselves if they could be in the images. The initial idea was to send a start-capture signal to the devices, and then a stop-capture signal once visitors entered the room. However, this was more time-consuming to set up than anticipated, as the choice was made to make them communicate over usb-mobile network connections.